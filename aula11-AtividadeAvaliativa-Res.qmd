# Aula 11 - Atividade Avaliativa - Resol. {#sec-Ativ-Avaliativa-Resol}

**Aluno**: Nome Do Aluno ...

Inciar a fase de **AED** (Análise Exploratória Descritiva) e de **AEI** (Análise Exploratória Inferencial).

## **Instruções**

-   O desenvolvimento desta atividade deve ser realizada de forma individual ou em grupo de 2 alunos.

-   Deve-se responder e completar este arquivo `Quarto Markdown` (`NomeAluno.qmd`) desta atividade.

-   É necessário enviar via WhatsApp o arquivo resposta renderizado em: `.qmd`, em `.pdf` e `.html`.

-   Valor da atividade: 10 pontos.

**Data disponível: 13/06/2024**

**Data máxima para entrega e *breve apresentação 1* em: 27/06/2024 (prazo de 2 semanas)**

**Data máxima para entrega e *breve apresentação 2* em: 11/07/2024 (prazo de + 2 semanas)**

**Atividades**: ***importar*** (leitura), ***armazenar*** (salvar; enclausurar), ***transformar*** (criar novas variáveis numéricas e categóricas), ***visualizar*** (descrever, plotar, capturar eventuais padrões), ***modelar*** (propor modelos explicativos) e ***testar*** (realizar testes de Hipótese) em relação aos dados primários levantados no corpus n. 3 desta pesquisa, que contém as ocorrências da DPAI Goiânia associadas aos jovens que vieram a obito: arquivo `obitjcsv.csv` , que se encontra na pasta `out` de nosso projeto `R`. Ou seja, estamos seguindo o ***conceito*** de **Ciclo da Ciência de Dados**:

[![Ciclo da Ciência de Dados, suas 3 fases: 1 - Wrangle (importar, organizar e transformar), 2 - Understand (Transformar, Visualizar e Modelar; buscar melhor ajuste) e 3 - Communicate (relatar) and Replicate (automatizar; app)](fig/2-data-science-wrangle-01.png){fig-alt="Conceito de Ciclo da Ciência de Dados" fig-align="center"}](https://r4ds.had.co.nz/wrangle-intro.html)

A fase ***Wrangle*** consome cerca de **80%** do tempo de uma pesquisa empírica [@Wickham2017R, p. ix, xi, 117]. O que foi realizado até o momento em nosso curso, uma tarefa, uma verdadeira luta, que não costuma nos agradar.

O mesmo ***conceito*** de **Ciclo da Ciência de Dados**, agora associado aos principais `pacotes` do `R` que auxiliam cada fase ou etapa dentro de cada fase: notamanete o pacote `tidyverse`.

[![O tidyverse é um pacote guarda-chuva que consolida uma série de ferramentas que fazem parte do ciclo da ciência de dados. Fazem parte do {tidyverse} os pacotes {ggplot2}, {dplyr}, {tidyr}, {purrr}, {readr}, entre muitos outros, como é possível observar na figura.](fig/Conceito-Pacotes-R-ciclo-ciencia-de-dados.png){fig-alt="Conceito de Ciclo da Ciência de Dados" fig-align="center"}](https://livro.curso-r.com/4-2-tidyverse.html)

Conferir uma ***cheat sheet*** mais completa em: <https://www.business-science.io/r-cheatsheet.html>.

Tudo isso casa com um mais amplo de **Conceito Ciclo da Ciência**, cujo formato costuma ser designado como ***Diamond Shape*** [@Donovan2019-ug , p. 274-275]:

![Conceito de Ciclo da Ciência na forma de um Diamond Shape (DONOVAN; MICKEY, 2019, p. 274-275))](fig/DiamondShape-Donovan.jpg){fig-alt="Ciclo da Ciência" fig-align="center"}

Nesta atividade avaliativa resolveremos alguns exercícios que fixarão os principais conceitos até aqui trabalhados nesta disciplina **EBRADPP**. O que não nos impede incursionar em conceitos simples de Estatística, como: média, desvio padrão, mediana, Amplitude Interquartil, correlação, regressão etc.

A ferramenta ***statkey*** é um bom aplicativo *free on line* para exercitar esses conceitos, experimente ela com nosso ***data set*** já organizado `obitjcsv.csv`, que se encontra na pasta `out` de nosso Projeto `EBRA-DPP.Rproj`; clique aqui: <https://www.lock5stat.com/StatKey/index.html>.

Mãos à obra pessoal, salientando que as próximas 2 fases de **AED** (Análise Exploratória Descritiva) e de **AEI** (Análise Exploratória Inferencial), que demandarão os **20%** restante do tempo de uma pesquisa, costumam ser-nos bem *mais prazerosas*.

Nessa nova fase, bem mais atraente, o objetivo é explorar os dados em busca do reconhecimento de padrões perceptíveis (cuidado com a possibilidade do *erro percepcional*).

*Resumos dos dados* são muito úteis: média e desvio padrão; mediana e AIQ (Amplitude Interquartil); resumo dos 5 números, coeficiente de variação, assimetria, curtose etc.

Gerar vários gráficos (histogramas, *boxplot*, pizza, barras, colunas, dispersão, diagrama de ramo e folha etc.) que permitam essa ***visualização*** e ***captura de padrões*** para cada variável observada, a depender do tipo de cada qual.

Bem como para investigar possibilidade de ***associação*** entre elas: a depender da combinação de tipos de cada uma delas, por meio de ***testes estatísticos*** formais.

Gerando tabelas de dupla entrada ou de contigência, quando ambas forem categóricas, do tipo **factor** \<`fctr`\>.

## Lista de Exercícios

Procure fazer uso dos pacotes do R acima mencionados.

Limpeza e *setup* do ambiente a ser utilizado.

```{r setup, include=TRUE}

# Deletar os objetos da Global Environment
rm(list=ls())

# Padrão de saídas Rmarkdown
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

# Instalar tidyverse caso não esteja já instalado
if (!require('tidyverse')) install.packages('tidyverse')
# Instalar pacote magrittr caso não esteja já instalado
if (!require("magrittr")) install.packages("magrittr")
# Instalar pacote mlr caso não esteja já instalado
if (!require("mlr")) install.packages("mlr")

# Carregar o pacote DBI na Global Environment: disponível para uso direto
library('tidyverse')
# Warning: package ‘tidyverse’ was built under R version 4.2.3
# Warning: package ‘ggplot2’ was built under R version 4.2.3
# Warning: package ‘tibble’ was built under R version 4.2.3
# Warning: package ‘tidyr’ was built under R version 4.2.3
# Warning: package ‘readr’ was built under R version 4.2.3
# Warning: package ‘purrr’ was built under R version 4.2.3
# Warning: package ‘dplyr’ was built under R version 4.2.3
# Warning: package ‘stringr’ was built under R version 4.2.3
# Warning: package ‘forcats’ was built under R version 4.2.3
# Warning: package ‘lubridate’ was built under R version 4.2.3
# ── Attaching core tidyverse packages # ──────────────────────────────────────────── tidyverse 2.0.0 ──
# ✔ dplyr     1.1.2     ✔ readr     2.1.5
# ✔ forcats   1.0.0     ✔ stringr   1.5.1
# ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
# ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
# ✔ purrr     1.0.2     
# ── Conflicts ────────────────────────────────────────────────────────────── # tidyverse_conflicts() ──
# ✖ dplyr::filter() masks stats::filter()
# ✖ dplyr::lag()    masks stats::lag()
# ℹ Use the conflicted package to force all conflicts to become errors

# Carregar o pacote magrittr na Global Environment: disponível para uso direto
library("magrittr")
# Attaching package: ‘magrittr’
# 
# The following object is masked from ‘package:purrr’:
# 
# set_names
# 
# The following object is masked from ‘package:tidyr’:
# 
#     extract

# Carregar o pacote mlr na Global Environment: disponível para uso direto
library("mlr")

# Carregar o pacote rmarkdown na Global Environment: disponível para uso direto
# library("rmarkdown")
```

### Item 1

**1**. **Importar** o *data set*, o arquivo `obitjcsv.csv`, que se encontra na pasta `out` de nosso Projeto `EBRA-DPP.Rproj`. Recomenda-se baixar a atualizar a última versão de nosso projeto que se encontra compartilhado no google drive: <https://drive.google.com/drive/u/1/folders/1wm9jUo5XlBHqbQDRf9XevFbXcqkWogqt>

```{r}

# Importar como tibble o arquivo de dentro da pasta chamada out.
obitj_csv <- readr::read_csv(file   = "out/obitjcsv.csv",
                             # delim  = ",",
                             quote  = "\"",
                             locale = locale(
                               decimal_mark = ".",
                               encoding     = "UTF-8"
                               )
                             )

cat("\n")
cat("Estrutura do objeto R denominado obitj_csv:\n")
str(obitj_csv)

cat("\n")
cat("Nomes das 24 colunas do objeto obitj_csv:\n")
names(obitj_csv)

obitj_csv # tibble:447 × 24
```

### Item 2

**2**. **Transformar** esse *data set* para que criar as seguintes variáveis categóricas:

***Transformar***, antes, as variáveis tipo `char` que enquadram-se como factor: `fctr`.

```{r}

# para explicitar a ordem das categorias nas variáveis
# que medem níveis de escolaridades: esc1 e esc2
# variável categórica ordinal com 12 levels
series <- c(
  "1 ano",
  "2 ano",
  "4 ano",
  "5 ano",
  "6 ano",
  "7 ano",
  "8 ano",
  "9 ano",
  "1 série EM",
  "2 série EM",
  "3 série EM"
  )

# para explicitar a ordem das categorias nas variáveis
# que medem apenas 2 níveis (levels): s - sim / n - não
# nessas ordem (e não na ordem alfabética)
sim_n <- c(
  "s",
  "n"
  )

# Declaração de Variáveis tipo char já existentes como categóricas
obitj_csv <- obitj_csv %>% 
  mutate(sexo =                   # nova variável tipo <fctr>
           sexo %>%               # a partir da variável original sexo
           factor() %>%           # converte para o tipo factor
           forcats::fct_recode(   # forcats função para recodificar labels
             "F" = "f",      # novo à esquerda, antigo à direita
             "M" = "m"),     # F = Feminino, M = Masculino
         
         # mesma coisa com código mais condensado:
         cor = factor(cor), # mantidos os levels originais: branco, pardo, preto
         
         # variável corag = cor agragada em apenas 2 categorias
         corag = factor(corag), # mantidos os levels originais: branco, negro
         
         # variável esc1 = escolaridade 1, com 11 categorias
         esc1 =                  # nova variável tipo <fctr>
           esc1 |>               # a partir da variável original esc2
           factor( series ) |>   # converte para o tipo factor: 11/12 categorias
           forcats::fct_recode(  # forcats função para recodificar labels
             "1ano" = "1 ano",   # novo à esquerda, antigo à direita
             "2ano" = "2 ano",
             "3ano" = "3 ano",   # embora esta categoria ñ ocorra
             "4ano" = "4 ano",
             "5ano" = "5 ano",
             "6ano" = "6 ano",
             "7ano" = "7 ano",
             "8ano" = "8 ano",
             "9ano" = "9 ano",
             "1serieEM" = "1 série EM",
             "2serieEM" = "2 série EM",
             "3serieEM" = "3 série EM"
           ), # mantido nenhum label original
         
         # variável esc2 = escolaridade 2, com 10 categorias
         esc2 =                  # nova variável tipo <fctr>
           esc2 |>               # a partir da variável original esc2
           factor( series ) |>   # converte para o tipo factor: 12 categorias
           forcats::fct_recode(  # forcats função para recodificar labels
             "1ano" = "1 ano",   # embora esta categoria ñ ocorra
             "2ano" = "2 ano",   # novo à esquerda, antigo à direita
             "3ano" = "3 ano",   # categoria que ñ ocorre
             "4ano" = "4 ano",
             "5ano" = "5 ano",
             "6ano" = "6 ano",
             "7ano" = "7 ano",
             "8ano" = "8 ano",
             "9ano" = "9 ano",
             "1serieEM" = "1 série EM",
             "2serieEM" = "2 série EM",
             "3serieEM" = "3 série EM"
           ), # mantido nenhum label original
         
         # variável compfam = composição familiar, com 6 categorias
         compfam =               # nova variável tipo <fctr>
           compfam |>            # a partir da variável original compfam
           factor() |>           # converte para o tipo factor: 6 categorias
           forcats::fct_recode(  # forcats função para recodificar labels
             "mae"      = "mãe",      # novo à esquerda, antigo à direita
             "mae_padr" = "mãe + padrasto",
             "pai_mae"  = "pai + mãe",
             "pai_madr" = "pai + madrasta",
           ), # mantidos só 2 labels originais: pai, parentes
         
         # variável relpai = relação com pai, com 3 categorias
         relpai =                # nova variável tipo <fctr>
           relpai |>             # a partir da variável original relpai
           factor() |>           # converte para o tipo factor: 3 categorias
           forcats::fct_recode(  # forcats função para recodificar labels
             "auxilio"     = "auxílio", # novo à esquerda, antigo à direita
             "mesma_resid" = "mesma residência",
             "ausente"     = "ausente"
           ), # mantidos só um label original: ausente

         # variável usudrog = usuário de droga, com 2 categorias: s / n
         usudrog =                 # nova variável tipo <fctr>
           usudrog |>              # a partir da variável original usudrog
           factor( sim_n ),        # converte para o tipo factor: 2 categorias
           
        # variável subst = Substância entorpecente, com 4 categorias
         subst =                 # nova variável tipo <fctr>
           subst %>%             # a partir da variável original subst
           factor() %>%          # converte para o tipo factor: 4 categorias
           forcats::fct_recode(  # forcats função para recodificar labels
             "coca_crack"  = "cocaína / crack", # novo à esquerda, antigo à direita
             "lsd_ecstasy" = "lsd, ecstasy",
             "licita"      = "lícitas"
           ), # mantidos só um level original: maconha
         
         # variável orgcrim = organização criminosa, com 2 categorias: s / n
         orgcrim =                 # nova variável tipo <fctr>
           orgcrim |>              # a partir da variável original orgcrim
           factor( sim_n ),        # converte para o tipo factor: 2 categorias
         
         # variável morte = tipo de morte, com 2 categorias: nat / viol
         morte = factor(morte),    # mantida ordem dos 2 levels originais
         
         # variável paf = morte por perfuração de arma de fogo, com 2 cat: s / n
         paf =                     # nova variável tipo <fctr>
           paf |>                  # a partir da variável original paf
           factor( sim_n ),        # converte para o tipo factor: 2 categorias
         
         # variável circobt = circunstância do óbito, com 5 categorias
         circobt =                # nova variável tipo <fctr>
           circobt %>%            # a partir da variável original circobt
           factor() %>%           # converte para o tipo factor: 5 categorias
           forcats::fct_recode(   # forcats função para recodificar labels
             "MDIP" = "intervenção policial", # novo à esquerda, antigo à direita
             # MDIP = Morte Decorrente Intervenção Policial
             "MDCC" = "conflitos entre criminalidade",
             # MDIP = Morte Decorrente Conflitos entre Criminalidade
             "transito"      = "trânsito", 
             "outros"        = "Outros",
             "conf_fam_afet" = "conflito familiar / afetivo"
             )
         )

obitj_csv |> 
  head(25)
```

Ummmmmm

Uma rápida inspeção em `esc1` e `esc2`.

Por meio de uma contagem das categorias presentes em `esc1`.

```{r}

obitj_csv |> 
  count(esc1)
```

Constata-se que a categoria `3ano` não ocorreu nos dados coletados para `esc1`.

E por meio de um gráfico de barras em `esc2`.

```{r}

obitj_csv |> 
  ggplot( aes(esc2) ) +
  geom_bar()
```

Constata-se que as categorias `1ano` e `3ano` não ocorreram nos dados coletados para `esc2`.

a.  variável tipo `fctr` denominada `esc3` com o seguinte escopo (`s` / `n`): jovem com menos que ensino fundamental completo (variável categórica `esc2`).

    ```{r}

    # Criar uma nova variável categórica: esc3 com 2 categorias (s / n)
    obitj_csv <- obitj_csv %>% 
      mutate(esc3 = # nova variável tipo <fctr>
               if_else( # a partir da variável original esc2
                 esc2 == "9ano" | esc2 == "1serieEM" | esc2 == "2serieEM" | esc2 == "3serieEM",
                 "n",  # não tem menos que ensino fundamental completo
                 "s",  # sim tem menos que ensino fundamental completo
                 ) %>% # converter para o tipo factor
               factor( c("s", "n") ), # explicitar ordem das 2 categorias
             .after = esc2 # inserir coluna logo após esc2
             )

    obitj_csv |> head(20)
    ```

b.  variável tipo `fctr` denominada `esc4` com o seguinte escopo (`s` / `n`): variável numérica `idadeobta` menor que sua média (18.7 anos) ***e*** jovem com menos que ensino fundamental completo (variável categórica `esc3`).

    ```{r}

    # Criar uma nova variável categórica: esc4 com 2 categorias (s / n)
    obitj_csv <- obitj_csv %>% 
      mutate(esc4 = # nova variável tipo <char>
               if_else( # a partir da variável original esc3
                 esc3 == "s" & idadeobta <= 18.7,
                 "s",  # sim: tem menos que ensino fundamental completo
                       #      e idade na data óbito <= 18.7 anos
                 "n"   # não: tem ensino fundamental completo ou mais
                       #     ou idade na data do seu óbito > 18.7 anos
                 ),    # deixar esc4 ainda com tipo char
               .after = esc3 # inserir coluna esc4 logo após esc3
             )

    obitj_csv <- obitj_csv %>% 
      mutate(esc4 =     # nova variável tipo <fctr>
               if_else( # quando as variáveis originais esc3 ou idadeobta forem NA
                 is.na(esc3) | is.na(idadeobta),
                 NA,    # esc4 também será NA
                 esc4   # caso contrário: seu valor será mantido
                 ) %>%  # converter esc4 para o tipo factor
               factor( levels = c("s", "n") ) # explicitar ordem das 2 categorias
             # já se encontra inserida em coluna logo após esc3
             )


    obitj_csv |>
      select(esc1, esc2, esc3, idadeobta, esc4) |> 
      head(30)
    ```

### Item 3

**3**. **Transformar** esse *data set* para criar a seguinte variável numérica:

Tempo afastado da escola na data do seu óbito, denominada `temposesc`, medida em anos: igual a `dataobt` menos a `dataesc1`.

Armazenar esse data frame em outra variável (`obitj`) do tipo `tibble` (mais amigável ao uso do pacote `dplyr`).

```{r}

# usar: mutate()  para criar essa variável temposesc (em anos)
# usar operador - para <date>: temposesc = dataobt - dataesc1

obitj_csv <- obitj_csv |> 
  # resultado em intervalo de tempo: número de dias
  dplyr::mutate(temposescd = (dataobt - dataesc1) ) |>
  # resultado em intervalo de tempo: número de anos <dbl>
  dplyr::mutate(temposesc = as.numeric(temposescd / 365.25) )

obitj <- as_tibble(obitj_csv) # desnecessário, pois já é um objeto
  # tibble:447 × 26, desde sua importação (item 1)

cat("\n")
cat("Classe do objeto obitj:\n")
class(obitj) # "tbl_df"     "tbl"        "data.frame"

obitj |> # A tibble:447 × 26, desde sua importação (item 1)
  head(30)
```

Uma boa estratégia é **exportar** essa `tiblle` no formato binário do R, na pasta `out`.

Pois na sua próxima ***importação*** ela virá com todos os seus tipos de colunas preservados: <char>, <date>, <time>, <fctr>, <dbl>.

Para evitar a repetição de todas essas transformações e criações de variáveis processadas até aqui.

Aproveitar para salvar, também, esta última versão da tabela em .csv, para eventual exploração no aplicativo [StatKey](https://www.lock5stat.com/StatKey/index.html "Statistics: Unlocking the Power of Data").

```{r}

# salvar essa tibble no formato binário do R na pasta out
# Sua próxima importação ela virá com todos os seus
# tipos de colunas preservados: <char>, <date>, <time>, <fctr>, <dbl>
write_rds(obitj, file = "out/obitj.rds")

# salvar arquivo obitj-v5.csv na pasta out
# Salvar uma planilha de dados toda tratada e transformada:
write.csv(obitj,
          file = "out/obitj-v5.csv",
          na   = "",         # salvar campos NA como espaço vazio <blank>
          row.names = FALSE) # não salvar coluna com números das linhas
```

### Item 4

**4**. **Visualizar** no R uma *tabela de dupla entrada* (tabela de contigência) entre as variáveis categóricas: `morte` (nat/viol) e `usudrog` (s/n).

```{r}

# tabela de contigência com R base:
obitj |> 
  select(morte, usudrog) |>
  table() |>  # nome da matriz é: usudrog
              # nome das colunas é: n    , s
              # nome das linhas  é: morte, nat e viol
  stats::addmargins() # adicionar somas marginais

# tabela de contigência com R base
# convertida em tibble para pelo dplyr:
obitj |> 
  select(morte, usudrog) |>
  table() |>
  as_tibble() |> 
  rownames_to_column() |> # transformar nome de linhas na coluna morte
  # e transformar nomes de colunas em categorias da coluna usudrog
  kableExtra::kable()

# utilizando somente o dplyr
obitj |> 
  group_by(morte, usudrog) |> # pivotar
  summarize( n=n() ) |>       # contar
  kableExtra::kable()         # exibir tabela no formato longo
```

Agora uma rápida visualização gráfica dessa tabela de contigência.

Gráfico Barras agrupadas escalonadas: `usudrog` x `morte`.

```{r}
#| label: fig-plot-barras-agrupadas-scale1-usudrog-morte
#| warning: false
#| fig-cap: "Gráfico de Barras agrupadas escalonadas: Usuário de droga (s/n) segundo o tipo de morte (nat/viol)\ndentre os jovens que vieram a óbito que\ntiveram passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 447)."

DF <- obitj |> 
  select(usudrog, morte) |> 
  filter(!is.na(usudrog)) |> 
  filter(!is.na(morte)) |> 
  group_by(usudrog, morte) |> 
  dplyr::summarize(n = dplyr::n())

tbl  <- xtabs(n ~ morte + usudrog, DF)

perc <- proportions(tbl, "morte")

tbl   <- as.data.frame.array(tbl)
perc  <- as.data.frame.array(perc)
#       s           n
#  nat  1.0000000   0.0000000
#  viol 0.7201018   0.2798982	  
vperc <- unlist(as.vector(perc))
#        s1        s2        n1        n2 
#        1.0000000 0.7201018 0.0000000 0.2798982 

obitj |> 
  select(usudrog, morte) |> 
  filter(!is.na(usudrog)) |> 
  filter(!is.na(morte)) |> 
  group_by(usudrog, morte) |> 
  dplyr::summarize(n = dplyr::n()) |> 
  # mutate_(pct = as.vector(perc)) |> 
  ggplot(aes(x = morte, y = n,
             fill = usudrog, group = morte)) +
  # scale_y_continuous(limits = c(0, 17.5), breaks = seq(0, 15, 5)) +
  geom_bar(stat = "identity",
           binwidth = 1,
           position = "fill",
           color = "black", 
           na.rm = TRUE) +
  scale_fill_grey(start = 0.0, end = 0.7) +
  # geom_text(aes(label = y/sum(n)), 
  #           position = position_stack(), vjust = -0.25) +
  geom_text(aes(y = vperc[-3],
                label = scales::percent(vperc[-3], accuracy = .1)),
            color = "white",
            position = position_stack(vjust = +0.5),
            size = 4) +
  labs(title    = "Gráfico de Barras Agrupadas Escalonadas: tipo de morte",
       subtitle = "Proporção (%) por tipo de morte natual ou violenta (N = 447)\nsegundo usuário de droga (s = sim / n = não).",
       y        = "Frequencia Relativa Escalonada",
       x        = "Tipo de morte (nat/viol)",
       caption  = "Fonte:  1. dados primários coletados por Queops (2024).")
```

Esse gráfico de barras agrupadas escalonadas ilustra que, embora contraintuitivo, dentre os jovens que vieram a óbito com passagem pela DEPAI de Goiânia, aqueles que apresentaram morte natural, todos eles eram usuário de droga; enquanto que dentre os que apresentaram morte violenta, 72% deles eram usuários de droga.

Esse resultado da pesquisa enfraquece a crença (intuitiva) de que a variável ser usuário de droga seria determinante para alcançar o tipo de morte violenta; outra possibilidade que não pode ser descartada é que dentre aqueles que tiveram morte violenta, uma possível autodeclaração quanto a ser ou não usuário de droga pode ter restado subnotificada.

### Item 5

**5**. **Visualizar** os seguintes gráficos:

a.  Gráfico de barras ou de setores (piza) para as variáveis categóricas: `sexo` e `cor`.

    Primeiro a variável categórica: `sexo`.

    ```{r}
    #| label: fig-plot-doughnut-plot-sexo
    #| fig-cap: "Doughnut plot: Contagem e Proporção (%) dos\njovens c/passagem na DEPAI de Goiânia por gênero (sexo)\nPeríodo: 2016 a 2023 (N = 447)."

    p1 <- obitj |> 
      select(nomean, sexo) |> 
      base::unique(nmax = 3, MARGIN = 0) |>
      group_by(sexo) |>
      dplyr::summarize(n = dplyr::n()) |> 
      arrange(desc(n)) |>
      mutate(p    = round(n / sum(n) * 100, 1),
             ypos = cumsum(p) - 0.5 * p) |>
      # Compute the cumulative percentages (top of each rectangle)
      mutate(ymax = cumsum(p)) |> 
      # Compute the bottom of each rectangle
      mutate(ymin = c(0, head(ymax, n = -1))
             ) |> 
      # Make the doughnut plot
      ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3,
                 fill = sexo)) +
         geom_rect(color = "black") +
         geom_text(aes(x = 3.5,
                       y = (ymax+ymin)/2,
                       label = paste0(as.character(n),"(" ,
                                      as.character(p), "%)")),
                   color = "white",
                   size = 4) +
         scale_fill_brewer(palette = "Set1") +
         coord_polar(theta = "y") + # Try to remove that to understand how the chart is built initially
         xlim(c(2, 4)) + # Try to remove that to see how to make a pie chart
         theme_void()

    p1 +
      guides(fill = guide_legend(title = "Sexo")) +
      labs(title    = "Doughnut plot: Contagem e Proporção (%) dos\njovens c/passagem na DEPAI de Goiânia por gênero (sexo)\nPeríodo: 2016 a 2023 (N = 438).",
           subtitle = "Contagem e Proporção (%) por sexo dos jovens.",
           x        = "",
           y        = "",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    Segundo a variável categórica: `cor`.

    ```{r}
    #| label: fig-plot-doughnut-plot-cor
    #| fig-cap: "Doughnut plot: Contagem e Proporção (%) dos\njovens c/passagem na DEPAI de Goiânia por cor\nPeríodo: 2016 a 2023 (N = 447)."

    p2 <- obitj |> 
      select(nomean, cor) |> 
      base::unique(nmax = 3, MARGIN = 0) |>
      group_by(cor) |>
      dplyr::summarize(n = dplyr::n()) |> 
      arrange(desc(n)) |>
      mutate(p    = round(n / sum(n) * 100, 1),
             ypos = cumsum(p) - 0.5 * p) |>
      # Compute the cumulative percentages (top of each rectangle)
      mutate(ymax = cumsum(p)) |> 
      # Compute the bottom of each rectangle
      mutate(ymin = c(0, head(ymax, n = -1))
             ) |> 
      # Make the doughnut plot
      ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3,
                 fill = cor)) +
         geom_rect(color = "black") +
         geom_text(aes(x = 3.5,
                       y = (ymax+ymin)/2,
                       label = paste0(as.character(n),"(" ,
                                      as.character(p), "%)")),
                   color = "black",
                   size = 4) +
         scale_fill_brewer(palette = "Set1") +
         coord_polar(theta = "y") + # Try to remove that to understand how the chart is built initially
         xlim(c(2, 4)) + # Try to remove that to see how to make a pie chart
         theme_void()

    p2 +
      guides(fill = guide_legend(title = "Cor")) +
      labs(title    = "Doughnut plot: Contagem e Proporção (%) dos\njovens c/passagem na DEPAI de Goiânia por cor\nPeríodo: 2016 a 2023 (N = 443).",
           subtitle = "Contagem e Proporção (%) por cor dos jovens.",
           x        = "",
           y        = "",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

b.  Histograma para as variáveis quantitativas: `idadeobta`, `npassag` e `temposesc`.

    Um histograma para cada uma das 3 variáveis.

    Histograma de : `idadeobta`.

    ```{r}

    # Variável idadeobta
    p3 <- obitj |> 
      select(idadeobta, npassag, temposesc) |> 
      ggplot(aes(x = idadeobta)) +
      geom_histogram()

    p3  +
      labs(title    = "Histograma: Idade na data do óbito dos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 440).",
           subtitle = "",
           x        = "anos",
           y        = "Freq. Absoluta",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    Há 3 `idadeobta` negativas e uma menor que 2 anos, na data do óbito. O que só pode ser atribuído a erros de coleta e lançamentos dos dados primários levantados com relação às seguintes observações:

    ```{r}

    obitj |> 
      filter(idadeobta < 2)
    ```

    Histograma de : `temposesc`.

    ```{r}

    # Variável temposesc
    p4 <- obitj |> 
      select(idadeobta, npassag, temposesc) |> 
      ggplot(aes(x = temposesc)) +
      geom_histogram(binwidth = 1) +
      scale_x_continuous(breaks = 0:10) +
      scale_y_continuous(breaks = seq(from = 0, to = 100, by = 10) )

    p4  +
      labs(title    = "Histograma: Tempo em anos fora da escola na data do óbito\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 403).",
           subtitle = "",
           x        = "anos",
           y        = "Freq. Absoluta",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    Há outras 3 observações com `temposesc` menor que zero, o que deve decorrer de erro de coleta ou lançamento de dados primários.

    ```{r}

    obitj |> 
      filter(temposesc < 0)
    ```

    Histograma de : `npassag`.

    ```{r}

    # Variável npassag
    p5 <- obitj |> 
      select(idadeobta, npassag, temposesc) |> 
      ggplot(aes(x = npassag)) +
      geom_histogram(binwidth = 1) +
      scale_x_continuous(breaks = 1:21) +
      scale_y_continuous(breaks = seq(from = 0, to = 80, by = 10) )

    p5  +
      labs(title    = "Histograma: Número de passagens até a data do óbito\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 447).",
           subtitle = "",
           x        = "Núm. Passagens",
           y        = "Freq. Absoluta",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    Histogramas lado a lado dessas 3 variáveis: `idadeobta`, `npassag` e `temposesc`.

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc) |> 
      filter(idadeobta >= 2) |> # filtrar as 4 observações inconsistentes
      filter(temposesc >= 0) |> # filtrar outras 3 observ. inconsistentes
      pivot_longer(cols = c("idadeobta", "npassag", "temposesc"),
                   names_to  = "variavel",
                   values_to = "anos_n",
                   values_drop_na = TRUE
                   ) |> 
      ggplot(aes(x = anos_n)) +
      geom_histogram() +
      facet_wrap("variavel", nrow = 1, scales = "free")
    ```

c.  *Boxplot* para as variáveis quantitativas: `idadeobta`, `npassag` e `temposesc`.

    *Boxplot* para a variável quantitativa: `idadeobta`.

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc) |>
      filter(idadeobta > 2) |> 
      ggplot( aes(x = idadeobta) ) +
        geom_boxplot()  +
      labs(title    = "Boxplot: Idade em anos na data do óbito\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 436).",
           subtitle = "",
           x        = "Idade (anos)",
           y        = "",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    *Boxplot* para a variável quantitativa: `npassag`.

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc) |>
      ggplot( aes(y = npassag) ) +
        geom_boxplot()  +
      labs(title    = "Boxplot: Número de passagens até a data do óbito\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 447).",
           subtitle = "",
           x        = "",
           y        = "Núm. Passagens",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    *Boxplot* para a variável quantitativa: `temposesc`.

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc) |>
      filter(temposesc >= 0) |>
      ggplot( aes(y = temposesc) ) +
        geom_boxplot()  +
      theme(
        axis.title.x = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank()
      ) +
      labs(title    = "Boxplot: Tempo fora da escola (anos) até a data do óbito\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 400).",
           subtitle = "",
           x        = "",
           y        = "Tempo fora Escola (anos)",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    Agora um gráfico boxplot lado a lado combinando duas variáveis: uma quantitativa no eixo y (`idadeobta`) e outra (eixo x) resultado da *transformação* de uma quantitativa em qualitativa (`npassag`)

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc) |>
      filter(idadeobta > 2) |> 
      ggplot( aes(x = npassag, y = idadeobta) ) +
        geom_boxplot( aes(group = cut_width(npassag, 1)) ) +
      scale_x_continuous(breaks = 0:21) +
      labs(title    = "Boxplot: idade (anos) na data do óbito por número de passagens\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 436).",
           subtitle = "",
           x        = "Número de Passagens",
           y        = "Idade (anos)",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

d.  Gráfico de barras lado a lado ou de barras empilhadas para verificar uma possível relação entre as variáveis categóricas: `morte` (nat/viol) e `usudrog` (s/n).

    ```{r}
    #| label: fig-plot-barras-lado-a-lado-morte-usudrog
    #| warning: false
    #| fig-cap: "Gráfico de Barras Lado a Lado: Tipo de morte dos jovens que vieram a óbito (natural/violenta) segundo uso de drogas (sim/não)\ncom passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 447)."

    obitj |> 
      select(morte, usudrog) |> 
      filter(!is.na(morte)) |> 
      filter(!is.na(usudrog)) |> 
      group_by(morte, usudrog) |> 
      dplyr::summarize(n = dplyr::n()) |> 
      ggplot(aes(x    = morte, y = n,
                 fill = usudrog, group = usudrog)) +
      scale_y_continuous(limits = c(0, 300), breaks = seq(0, 300, 50)) +
      geom_bar(stat = "identity", binwidth = 1,
               position = "dodge",
               color = "black", 
               na.rm = TRUE) +
      scale_fill_grey(start = 0.0, end = 0.7) +
      geom_text(aes(label = n), 
                position = position_dodge(width = 0.9), vjust = -0.25) +
      geom_text(aes(y = n - 0.5,
                    label = scales::percent(round(n/sum(n), 3))),
                color = "white",
                position = position_dodge(width = 0.9), vjust = +1.0,
                size = 3) +
      labs(title    = "Gráfico de Barras Lado a Lado: tipo de morte",
           subtitle = "Quantidade por tipo de morte natual ou violenta (N = 399)\nsegundo usuário de droga (n = não / s = sim).",
           y        = "Frequencia Abs./Relativa",
           x        = "Tipo de morte (natural/violenta)",
           caption  = "Fonte:  1. dados primários coletados por Queops (2024).")
    ```

e.  *Boxplot*, lado a lado, para as variáveis quantitativas: `idadeobta` e `npassag`, agrupadas segundo a variável categórica `corag` (cor agregada).

    *Boxplot*, lado a lado, para a variável quantitativa `idadeobta`, agrupada segundo a variável categórica `corag` (cor agregada).

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc, corag) |>
      filter(idadeobta > 2) |> 
      # filter( !is.na(corag) ) |> 
      ggplot( aes(x = corag, y = idadeobta) ) +
        geom_boxplot( aes(group = corag) ) +
      labs(title    = "Boxplot: idade (anos) na data do óbito por cor agregada (branco/negro/NA)\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 436).",
           subtitle = "",
           x        = "Cor agregada (branco/negro)",
           y        = "Idade (anos)",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

    *Boxplot*, lado a lado, para a variável quantitativa `npassag`, agrupada segundo a variável categórica `corag` (cor agregada).

    ```{r}

    obitj |> 
      select(idadeobta, npassag, temposesc, corag) |>
      filter(npassag >= 0) |> 
      # filter( !is.na(corag) ) |> 
      ggplot( aes(x = corag, y = npassag) ) +
        geom_boxplot( aes(group = corag) ) +
      labs(title    = "Boxplot: Número de passagens até a data do óbito por cor agregada (branco/negro/NA)\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 447).",
           subtitle = "",
           x        = "Cor agregada (branco/negro)",
           y        = "Número de passagens (n)",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

f.  Diagrama de *dispersão* para duas variáveis quantitativas: `idadeobta` e `npassag`, acrescido com sua reta de regressão (método dos mínimos quadrados).

    ```{r}

    obitj |> 
      filter(idadeobta > 2) |> 
      ggplot( aes(x = npassag,
                  y = idadeobta) ) +
      geom_point() +
      geom_smooth(se = TRUE, method = "lm") +
      labs(title    = "Diagrama de Dispersão: Idade na data do óbito por Número de passagens\ndos jovens c/passagem na DEPAI de Goiânia (c/reta regressão)\nPeríodo: 2016 a 2023 (N = 436).",
           subtitle = "",
           x        = "Número de passagens (n)",
           y        = "Idade (anos)",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

g.  Diagrama de *dispersão* para duas variáveis quantitativas: `idadeobta` e `temposesc`, acrescido com sua reta de regressão (método dos mínimos quadrados).

    ```{r}

    obitj |> 
      filter(idadeobta >  2) |> 
      filter(temposesc >= 0) |> 
      ggplot( aes(x = temposesc,
                  y = idadeobta) ) +
      geom_point() +
      geom_smooth(se = TRUE, method = "lm") +
      labs(title    = "Diagrama de Dispersão: Idade na data do óbito por Tempo fora da escola\ndos jovens c/passagem na DEPAI de Goiânia (c/reta regressão)\nPeríodo: 2016 a 2023 (N = 396).",
           subtitle = "",
           x        = "Tempo fora escola (anos)",
           y        = "Idade (anos)",
           caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
           )
    ```

### Item 6

**6**. **Modelar** no R uma *regressão linear simples* entre as seguintes variáveis quantitativas, dependente e independente:

a.  `idadeobta` e `npassag`.

    ```{r}

    modelo1 <- lm(formula = idadeobta ~ npassag,
                  data    = obitj[obitj$idadeobta > 2, ])

    summary(modelo1)
    ```

No modelo acima foram filtradas `idadeobita` negativas e menores ou iguais a 2 anos.

b.  `idadeobta` e `temposesc`.

    ```{r}

    modelo2 <- lm(formula = idadeobta ~ temposesc,
                  data    = obitj[obitj$idadeobta > 2 & obitj$temposesc >= 0, ])

    summary(modelo2)
    ```

No modelo acima foram filtradas `idadeobita` maior que 2 anos ***e*** `temposesc` maior ou igual a zero.

### Item 7

**7**. No R, realizar uma ***análise dos resíduos*** dos dois modelos do item 6 e decidir qual dos dois tem maior capacidade de explicar a variabilidade observada na variável dependente.

Análise dos resíduos do modelo 1: `idadeobta` x `npassag`

```{r}

plot(modelo1)
```

Análise dos resíduos do modelo 2: `idadeobta` x `temposesc`

```{r}

plot(modelo2)
```

### Item 8 (opcional)

**8**. No R, realizar uma ***regressão linear múltipla*** e sua respectiva ***análise dos resíduos*** de alguns modelos a partir do *data set* obitido como produto dos itens n. 2 e 3 e decidir qual deles teria maior capacidade de explicar a variabilidade observada na variável dependente.

Apoiar essa decisão em testes de Hipótese, como, por exemplo, os ilustrados no quadro a seguir [@Becker2015 , p. 458]:

![Testes recomendados para testar a significância da relação entre duas variáveis e respectivos níveis de mensuração, principais suposições teóricas e procedimentos alternativos](fig/Testes-Recomendados-JLBecker-2015-p-458.JPG){fig-align="center"}

Insira sua resposta no *chunk* a seguir.

```{r}

modelo3 <- glm(formula = idadeobta ~ temposesc + npassag,
               data    = obitj[obitj$idadeobta > 2 & obitj$temposesc >= 0, ])

summary(modelo3)

plot(modelo3)
```

Um modelo 4 com as variáveis explicativas: `temposesc`, `npassag` e `esc4`.

```{r}

modelo4 <- glm(formula = idadeobta ~ temposesc + npassag + esc4,
               data    = obitj[obitj$idadeobta > 2 & obitj$temposesc >= 0, ])

summary(modelo4)

plot(modelo4)
```

Uma boa olhada em uma matriz de dispersão de todas as variáveis de `obitj`.

```{r}

library(GGally)
library(plotly)

p <- obitj |> 
  select(idadeobta, temposesc, npassag, esc4, sexo, usudrog, corag) |> 
  filter(idadeobta >  2) |> 
  filter(temposesc >= 0) |> 
  ggpairs(title = "Correlograma com ggpairs()")

ggplotly(p)
```

Explorar um pouco mais as possibilidades explicativas do quadro de variáveis.

```{r}

library(ggridges)

obitj |> 
  filter(idadeobta >  2) |> 
  filter(temposesc >= 0) |> 
  filter( !is.na(esc4) ) |> 
  ggplot(
       aes(x = idadeobta,
           y = esc4, fill = esc4, color = esc4)) +
  geom_density_ridges(alpha = 0.5, show.legend = FALSE) +
  scale_x_continuous(breaks = 13:28) +
  labs(title    = "Densidade de probabilidade: Idade na data do óbito abaixo da média (18.7 anos) sem\nensino fund. compl. (var. esc4) dos jovens c/passagem na DEPAI de Goiânia",
       subtitle = "Período: 2016 a 2023 (N = 379)",
       x        = "Idade (anos)",
       y        = "esc4 (sim/não)",
       caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
       )
# Picking joint bandwidth of 0.431


obitj |> 
  filter(idadeobta >  2) |> 
  filter(temposesc >= 0) |> 
  filter( !is.na(circobt) ) |> 
  ggplot(
       aes(x = idadeobta,
           y = circobt, fill = circobt, color = circobt)) +
  geom_density_ridges(alpha = 0.5, show.legend = FALSE) +
  scale_x_continuous(breaks = 12:29) +
  labs(title    = "Densidade de probabilidade: Idade na data do óbito agrupada pela\ncircunstância do óbito dos jovens c/passagem na DEPAI de Goiânia",
       subtitle = "Período: 2016 a 2023 (N = 391)",
       x        = "Idade (anos)",
       y        = "circobtj",
       caption  = "Fonte: Dados primários coletados por Queops (2024)\nMDIP - Morte Decorrente Intervenção Policial\nMDCC - Morte Decorrente Conflitos entre Criminalidade"
       )
# Picking joint bandwidth of 0.794
# MDIP = Morte Decorrente Intervenção Policial
# MDCC = Morte Decorrente Conflitos entre Criminalidade

obitj <- obitj |> 
  mutate(anobit = year(dataobt) )

obitj |> 
  filter(idadeobta >  2) |> 
  filter(temposesc >= 0) |> 
  filter( !is.na(idadeobta) ) |> 
  filter( !is.na(anobit) ) |> 
  mutate(anobit = as.factor(anobit) ) |> 
  ggplot(
       aes(x = idadeobta,
           y = anobit, fill = anobit, color = anobit)) +
  geom_density_ridges(aes(y = anobit),
                      alpha = 0.5, show.legend = FALSE) +
  scale_x_continuous(breaks = 12:29) +
  labs(title    = "Densidade de probabilidade: Idade na data do óbito agrupada pelo\nano do óbito dos jovens c/passagem na DEPAI de Goiânia",
       subtitle = "Período: 2016 a 2023 (N = 396)",
       x        = "Idade (anos)",
       y        = "ano",
       caption  = "Fonte: Dados primários coletados por Queops (2024)"
       )
# Picking joint bandwidth of 0.794
# MDIP = Morte Decorrente Intervenção Policial
# MDCC = Morte Decorrente Conflitos entre Criminalidade
```

Um curioso gráfico de Cleveland em ordem decrescente dos Bairros de domicílio dos jovens com passagem na DEPAI que vieram a obito.

```{r}

obitj |> 
  mutate(dom = factor (dom) ) |>
  filter(!is.na(dom)) |> # n = 160 com Bairros dos domicílios registrados
  group_by(dom) |> 
  mutate(n = n(), .after = dom) |> 
  mutate(dom = forcats::fct_reorder(dom, n)) |> 
  select(dom, n) |>
  arrange(desc(n)) |> 
  unique() |> 
  head(25) |> 
  ggplot(aes(x = n,
             y = forcats::fct_reorder(dom, n) ) ) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = 1:4) +
  theme(axis.text  = element_text(size = 10),
        axis.title = element_text(size = 14, face = "bold")) +
  labs(title    = "Diagrama de Cleveland: Bairros do domicílio do jovem por número de óbitos (decrescente)\ndos jovens c/passagem na DEPAI de Goiânia\nPeríodo: 2016 a 2023 (N = 160 bairros; n = primeiros 25 bairros).",
       subtitle = "",
       x        = "n",
       y        = "Bairro do domicílio",
       caption  = "Fonte: 1. dados primários coletados por Queops (2024)."
       )
```

Quiçá uma automação do processo de análise de possíveis componentes de uma regressão linear simples.

## **Selecting the Best Predictors for Linear Regression in R**

Fonte, clique: [RPubs](https://rpubs.com/Tatjana_Kec/1014475 "Lecture_7: Subset Variable Selection").

### **Variable Selection with `Leaps`**

Perform Linear Regression with All Predictors

```{r}

names(obitj)
colunas <- c("sexo"     , "cor"     , "corag"    , 
             "esc1"     , "esc2"    , "esc3"     , "esc4", 
             "compfam"  , "relpai"  , "usudrog"  , "subst",
             "orgcrim"  , # "morte"   , "paf"      , "circobt",
             "idadeobta",  "npassag", "temposesc")
lm1 <- lm(obitj[, colunas] |> 
            filter(idadeobta >  2) |> 
            filter(temposesc >= 0) |> 
            filter( !is.na(esc4) ),
          formula = idadeobta ~ .)

summary(lm1)

# coragnegro                  NA         NA      NA       NA
```

Install and load the leaps library

```{r}

library(leaps)
```

Run the regsubsets() function on all variables.

```{r}

Best_Subset <-
    regsubsets(idadeobta ~ .,
               data  = obitj[, colunas] |> 
                 filter(idadeobta >  2) |> 
                 filter(temposesc >= 0) |> 
                 filter( !is.na(esc4) ),
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL,
               force.out = NULL,
               method = "exhaustive")

summary(Best_Subset) # tempo processando > 1 min.
                     # 41 Variables  (and intercept)

# as.data.frame(summary_best_subset$outmat)
```

Number of Predictors: 15

```{r}

summary_best_subset <- summary(Best_Subset)

which.max(summary_best_subset$adjr2)

# summary_best_subset$outmat

coef(Best_Subset, 15)
```

Seems like we have to use ***15 predictors*** and ***intercept*** to get the best model.

Control max number of exploratory variables.

```{r}

Best_Subset_15 <- 
  regsubsets(idadeobta ~ .,
               data  = obitj[, colunas] |> 
                 filter(idadeobta >  2) |> 
                 filter(temposesc >= 0) |> 
                 filter( !is.na(esc4) ),
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = 15,      # Limit on number of variables is 15
               force.in = NULL,
               force.out = NULL,
               method = "exhaustive")

summary_best_subset_15 <- summary(Best_Subset_15) # tempo processando = 1 min!
summary_best_subset_15

which.max(summary_best_subset_15$adjr2)

coef(Best_Subset_15, 15)

coef(Best_Subset_15, 15) |> 
  names()
```

Summary table below provides details on which predictors to use for the model. The best predictors are indicated by ‘TRUE’.

```{r}

var_signif <- summary_best_subset$which[15, ] |> 
  as.data.frame() |> 
  rownames_to_column(var = "var") |> 
  rename(in_model = "summary_best_subset$which[15, ]") |> 
  filter(in_model == TRUE) |> 
  rowid_to_column(var = "n")

var_signif |> 
  kableExtra::kable()
```

Compare different error indexes.

Remember that we can use the $\bar{R}^2$ (*Mean* *Residual Sum of Squares*) to select the best model.

The **adjusted** $R^2$ represents the proportion of variation, in the outcome, that are explained by the variation in predictors values.

We need to discover the other pieces of information contained in the `summary_best_subset`.

The `summary()` function also returns the necessary statistics for the best model selection.

It provides $R^2$, $RSS$ (*Residual Sum of Squares*), $\bar{R}^2$(*Mean* *Residual Sum of Squares* or *S* - Square root of *MSE*), *Mallows'* $C_p$, and $BIC$ (*Bayesian Information Criteria*), which will help us in the selection of the best overall model.

The $R^2$ statistic is defined as:

$$
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}
$$

Knowing that the $R^2$ statistic increases monotonically as more variables are included, it will not be effective to use it in the model selection procedure. However, we are going to examine it to see the level to which it increases.

```{r}

names(summary_best_subset)

t(t(sprintf("%0.2f%%", summary_best_subset$rsq * 100)))
```

Creating the line plots to visualise $RSS$ (*Residual Sum of Squares*), $\bar{R}^2$, $C_p$, and $BIC$ (*Bayesian Information Criteria*), for all of the models at once will help us to decide which model to select. We will keep things simple and create the visualisations using the `plot` function in `base` R with argument option `type = "l"` onto which we will superimpose the corresponding minimal or maximal values using the `point` function in respect of the statistic displayed.

```{r}

par(mfrow = c(2,2)) # Set up a 2x2 grid for display of 4 plots at once

plot(summary_best_subset$rss,
     xlab = "Number of Variables",
     ylab = "RSS",
     type = "l")
 
# line plot of adjusted R^2 statistic
plot(summary_best_subset$adjr2,
     xlab = "Number of Variables",
     ylab = "Adjusted RSq",
     type = "l")

# identify the location of the maximum point
adj_r2_max <- which.max(summary_best_subset$adjr2)

# plot a red point to indicate the model with the largest adjusted R^2 statistic
points(adj_r2_max,
       summary_best_subset$adjr2[adj_r2_max],
       col ="red",
       cex = 1,
       pch = 20)
 
# line plot of C_p and BIC, but this time in a search of the models with the SMALLEST statistic
# line plot of C_p statistic
plot(summary_best_subset$cp,
     xlab = "Number of Variables",
     ylab = "Cp",
     type = "l")

cp_min <- which.min(summary_best_subset$cp)
points(cp_min,
       summary_best_subset$cp[cp_min],
       col = "red",
       cex = 1,
       pch = 20)

# line plot of BIC statistic
plot(summary_best_subset$bic,
     xlab = "Number of Variables",
     ylab = "BIC",
     type = "l")

bic_min <- which.min(summary_best_subset$bic)
points(bic_min,
       summary_best_subset$bic[bic_min],
       col = "red",
       cex = 1,
       pch = 20)
```

We see that the measures are not in sync with one another and we realise that no one measure is going to give us an entirely accurate picture.

According to adjusted $R^2$ (16) and $C_p$ the best performing model is the one with: 13 variables.

And according to $BIC$ the best performing model has only 5 variables.

This outcome suggests that the models with ***fewer than 5 predictors*** is insufficient, ***underfit***.

While a model with ***more than 13 predictors*** will ***overfit***.

The `regsubsets()` function has a built-in `plot()` command which can be used to display the selected variables for the best model with a given number of predictors, ***ranked*** according to the $R^2$, $\bar{R}^2$, $C_p$, and $BIC$ statistic.

*The **top row*** of each plot contains a ***black square*** for each variable selected according to the optimal model associated with that statistic.

That is, when creating this plot for the display of the $R^2$ value it is not a surprise to see that the top row indicates inclusion of **all 15 predictors** for the $R^2$ of 82.4%.

```{r}

plot(Best_Subset_15, scale = "r2")
```

We can now identify the suggested **13** or **14** predictors for the best model by observing the top row and checking which of the variables has a black square.

This occur in the case of $R^2$ and $C_p$ analisys, for the following **13 predictors**: `esc14ano`, `esc24ano`, `esc25ano`, `esc27ano`, `esc4n`, `compfampai_mae`, `compfampai_parentes`, `substlsd_ecstasy`, `temposesc`, `esc12ano`, `esc13serieEM`, `esc3n` e `substlicita`.

In the following plot by observing the top row and checking which of the variable has a black square we can identify the exact 14 or 15 predictors to be used for the best model as suggested by $\bar{R}^2$.

```{r}

plot(Best_Subset_15, scale = "adjr2")
```

It suggest the following ***15 predictors*** model:

$idadeobta = f(esc12ano, esc14ano, esc24ano, esc25ano, esc26ano, esc27ano, esc4n, compfampai\_mae,$

$$
\;compfamparentes, substlsd\_ecstasy, temposesc, esc13serieEM, esc23serieEM, esc3n, substlicita)
$$

We can use the `coef()` function to see the coefficient estimates associated with this model.

```{r}

coef(Best_Subset_15, 15)
```

$C_p$ suggests the same set of predictors as $\bar{R}^2$.

```{r}

plot(Best_Subset_15, scale = "Cp")
```

Finally, with $BIC$ (*Bayesian Information Criteria*).

```{r}

plot(Best_Subset_15, scale = "bic")
```

We see that two models share a *BIC* close to -440.

Getting only 5 predictors: `esc24ano`, `esc3n`, `compfampai_mae`, `compfamparentes` and `substlicita`.

However, the model with the lowest *BIC* is the **12-variable model** that contains only: `esc12ano`, `esc14ano`, `esc24ano`, `esc25ano`, `esc13serieEM`, `esc3n`, `compfampai_mae`, `compfamparentes`, `npassag`, `temposesc`, `substlicita` and `substlsd_ecstasy`.

The other one perform **13-variable model** that contains ***all 12*** and: `esc4n`.

```{r}

cat("Modelo com 5 variávies\n")
coef(Best_Subset_15, 5)

cat("\n")
cat("Modelo com 6 variávies\n")
coef(Best_Subset_15, 6)

cat("\n")
cat("Modelo com 7 variávies\n")
coef(Best_Subset_15, 7)

cat("\n")
cat("Modelo com 8 variávies\n")
coef(Best_Subset_15, 8)

cat("\n")
cat("Modelo com 9 variávies\n")
coef(Best_Subset_15, 9)

cat("\n")
cat("Modelo com 10 variávies\n")
coef(Best_Subset_15, 10)

cat("\n")
cat("Modelo com 11 variávies\n")
coef(Best_Subset_15, 11)

cat("\n")
cat("Modelo com 12 variávies\n")
coef(Best_Subset_15, 12)

cat("\n")
cat("Modelo com 13 variávies\n")
coef(Best_Subset_15, 13)
```

### Run the regression model with the best 5 to 13 predictors

Considerando que `esc1` e `esc2` podem apresentar colineariedade, optaremos apenas por `esc2`.

E `esc3`, em vez de `esc4`, pela mesma razão.

```{r}

var_signif$var[-1]

# [1] "coragnegro"      "esc14ano"        "esc22ano"        "esc26ano"        "esc29ano"
# [6] "esc3s"           "esc4s"           "compfampai"      "compfampai_mae"  "compfamparentes"
# [11] "relpaiauxilio"   "usudrogs"        "substmaconha"    "substoutras"     "npassag"
# [16] "temposesc"

# x <- "coragnegro" + "esc12 série EM" + "esc13 série EM" + "esc19 ano" + "esc22 ano" + "esc24 ano" + "esc29 ano" + "esc3s" + "esc4s" + "compfampai + mãe" + "substlícitas" + "substmaconha" + "substoutras" + "npassag" + "temposesc"

# coragnegro                  NA         NA      NA       NA
# excluir: coragnegro
best.model <- lm(idadeobta ~ esc2 + compfam + subst + npassag + temposesc,
                 data = obitj[, ] |> 
                 filter(idadeobta >  2) |> 
                 filter(temposesc >= 0) |> 
                 filter( !is.na(esc2) )
                 )

summary(best.model)
```

Rodou todo o script em aproximadamente 1 min 25 s.

```{r}

broom::glance(best.model)

plot(best.model)
```

Usando o pacote `MASS` para autodeterminição do melhor modelo de regressão linear múltipla.

```{r}

library(MASS)

lm_full <- lm(obitj[, colunas] |> 
             filter(idadeobta >  2) |> 
             filter(temposesc >= 0) |>
             na.omit(),
             # filter( !is.na(esc4) ),
             formula = idadeobta ~ .,
             na.action = na.omit)


modelo_AIC <- MASS::stepAIC(lm_full,
                            direction = "both")
modelo_AIC
```

Exibir o modelo que com menor AIC que foi selecionado.

Equação do modelo, para NIvel Confiança = 95% (Erro Tipo I = 5%), é:

$$
idadeobta = 15.4 + 1.29 \cdot esc4n - 0.75 \cdot substlicita - 0.49 \cdot substmaconha - 0.73 \cdot substoutras +
$$

$$
... + 0.05 \cdot npassag + 0.84 \cdot temposesc - 0.67 \cdot esc3n + 1.07 \cdot esc19ano + 1.15 \cdot esc11serieEM +
$$

$$ ... + 1.63 \cdot esc12serieEM + 2.22 \cdot esc13serieEM$$

Agora é possível predizer a idade na data do provável óbito do seguinte jovem que acaba de ter passagem resgitrada na DEPAI:

-   idade = 19.5 anos, sexo masculino, cor parda, que já concluiu 1serieEM;

-   esc4: não; pois ele tem mais de 18.7 anos;

-   usuário de: maconha;

-   npassag: em sua 3ª passagem pela DEPAI;

-   temposesc: há 1 ano fora da escola;

-   esc3: não; pois ele tem mais que o ensino fundamental completo.

É possível estimar, com 95% de Nível de Confiança, sua provável idade na data do óbito pelo modelo de regressão linear escolhido:

$$
idadeobita = 15.4 + 1.29 \cdot 1 - 0.49 \cdot 1 + 0.05 \cdot 3 + 0.84 \cdot 1 - 0.67 \cdot 1 + 1.15 \cdot 1 = 17.67 \; anos
$$

Como esse jovem tem 19.5 anos, seria esperado que já tivesse vido a óbito há: -1.83 anos atrás (resíduo).

Ou seja, o modelo fez sua predição dentro de uma margem de erro menor que 2 anos. Exibir esse modelo escolhido, a seguir:

```{r}


lm_AIC <- lm(obitj[, colunas] |> 
             filter(idadeobta >  2) |> 
             filter(temposesc >= 0) |>
             na.omit(),
             formula = idadeobta ~ esc4 + subst + npassag + temposesc + esc3 + esc1 + corag,
             na.action = na.omit)

summary(lm_AIC)

AIC(lm_AIC)

plot(lm_AIC)

df <- lm_AIC$residuals |> 
  as.data.frame() |> 
  rename(
    "residuos" = "lm_AIC$residuals"
  )

df |> 
  ggplot(aes(x = residuos)) +
  geom_histogram(aes(y =..density..),
                 breaks = seq(-3, 3, by = 0.25), 
                 colour = "black", 
                 fill = "white") +
  stat_function(fun = dnorm,
                args = list(mean = mean(df$residuos),
                            sd   = sd  (df$residuos)),
                color = "blue", size = 0.1) +
  geom_density(aes(x = df$residuos)) +
  scale_x_continuous(breaks = seq(-3, 3, by = 1.0))

olsrr::ols_plot_cooksd_chart(lm_AIC)

olsrr::ols_plot_resid_lev(lm_AIC)
```

Cuja distribuição dos resíduos é razoavelmente próxima a uma distribuição Normal. Mas é necessário ***testar essa hipótese***, pelo teste de Shapiro-Wilks.

> Diferentes testes de normalidades aplicados à mesma amostra/população, mas a escolha por um deles deve ser baseada no tamanho amostral: o teste de Anderson-Darling é mais poderoso para amostras maiores que 50, enquanto o de Shapiro-Wilk é mais poderoso para amostras menores ou iguais a 50. O teste K-S (Lilliefors) é o menos poderoso. cf. [link](www.est.ufmg.br/portal/wp-content/uploads/2023/01/RTE_02_2018.pdf "Estatística não paramétrica básica no software R: uma abordagem por resolução de problemas")

A seguir o teste de Anderson-Darling, pois a população modelada conta com N = 298 observações.

```{r}

library(nortest)

ad.test(df$residuos)

shapiro.test(df$residuos)

ks.test(df$residuos, rnorm(n = 298,
                           mean = 0.0,
                           sd = sd(df$residuos)) )
```

Todavia esse teste, com valor-p = 0.016 = 1.6% \< 5% (Erro Tipo I), indica que a estatística de teste caiu na Região de Rejeição, o que leva à ***rejeição*** da hipótese nula: Ho - 𝑂𝑠 𝑑𝑎𝑑𝑜𝑠 𝑠𝑒𝑔𝑢𝑒𝑚 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑖çã𝑜 𝑁𝑜𝑟𝑚𝑎𝑙*.* O teste de Shapiro-Wilk também leva à rejeição dessa hipótese nula (NC = 95%).

Todavia, um teste de Kolmogorov-Smirnov, que considera as duas distribuições testadas contínuas, exibiu um valor-p = 0.45 = 45% \> 5%, caindo fora da região de rejeição; por esse teste *não* se deveria rejeitar a hipótese nula de nenhuma diferença entre as duas distribuições testadas: os resíduos do modelo e uma distribuição normal com média zero e desvio padrão igual ao desvio padrão observado nos dados: `sd = 0.8618297 anos`.

Ou seja, há forte evidência a favor da hipótese alternativa, de que, com 95% de confiança, os resíduos do modelo escolhido `lm_AIC` ***não*** seguem uma distribuição normal, que é um pressuposto de um modelo de regressão linear.

O que exige maior aprofundamento na construção e escolha desse modelo.

### LASSO

***LASSO*** - ***Least Absolute Shrinkage and Selection Operator***

Conferir o seguinte livro clássicoe aprofundado para uma Introdução ao Aprendizado com Estatística:

[![An Introduction to Statistical Learning (ISLR) - with applications in R 2nd ed.](fig/AISL.png){fig-alt="ISLR with R 2nd edition" fig-align="center" width="268"}](https://www.statlearning.com/)

A seguir uma implementação para uma regressão LASSO para o data set `obitj`, utilizando a técnica do *Cross Validation* para separar o data set em 2 subconjuntos: `train` e `test`, selecionados aleatoriamente.

```{r}

# Seleção apenas das seguintes 15 variáveis de interesse:
# [1] "sexo"      "cor"       "corag"     "esc1"      "esc2"      "esc3"
#     "esc4"      "compfam"
# [9] "relpai"    "usudrog"   "subst"     "orgcrim"   "idadeobta" "npassag"
#     "temposesc"

obitj_red <- obitj[, colunas] |> # A tibble: 298 obs. × 15 vars.
               filter(idadeobta >  2) |> 
               filter(temposesc >= 0) |>
               na.omit()

# Variável resposta:
y <- obitj_red$idadeobta

# matriz de variáveis independentes numéricas
# transforma todas as categóricas em variáveis dummy (0, 1)
# excluir coluna n. 1: intercepto
x <- stats::model.matrix(idadeobta ~ ., data = obitj_red)[, -1]

library(glmnet)

grid <- 10^seq(10, -2, length = 100)

# Cross Validation
set.seed(1)
# Conjunto de treino do modelo com metade do data set: aleatoriamente selecionado
train  <- sample(1:nrow(x), nrow(x) / 2) # 298/2 = 149 obs. escolhidas alea
# a outra metade (complementar), fica para o subconjunto de teste
test   <- (-train) # as outras 149 obs. complementares
y.test <- y[test]

# test set MSE - Mean Squared Error of the test set
cat("test set of the null model MSE = ")
mean((mean(y[train]) - y.test)^2) # em anos ao quadrado
cat("anos ao quadrado\n\n\n")


lasso.mod <- glmnet(x[train , ], y[train],
                    alpha = 1, lambda = grid)
plot(lasso.mod)

# Croos validation check
set.seed(1)
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod , s = bestlam ,newx = x[test , ])

# compute the associated test error: test set MSE
cat("test set MSE = ")
mean((lasso.pred - y.test)^2)
cat("anos ao quadrado\n")
```

The ***test set MSE*** of 1.01 (anos ao quadrado) is substantially lower than the ***test set MSE of the null model*** and of least squares: 4.49 anos.

Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with λ chosen by cross-validation contains only eleven variables.

```{r}

out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out , type = "coefficients", s = bestlam)[, ]
length(lasso.coef) # 42 variáveis
lasso.coef |> 
  as.data.frame() |> 
  rownames_to_column()

lasso.coef |> 
  as.data.frame() |> 
  rownames_to_column() |> 
  as_tibble() |> 
  filter(lasso.coef != 0) # modelo com 21 variáveis (2 quant e 19 dummy)
                          # modelo de regressão com 12 variáveis.
# 12 variáveis, a saber: 10 categóricas 2 quantitativas
# cor, corag, esc1, esc2, esc3, esc4, compfam, relpai, usudrog, subst,
# npassag, temposesc

# summary(lm(log(y) ~ x))
lm_lasso_1 <- lm(    y  ~ x)
summary(lm_lasso_1)

AIC(lm_lasso_1)
```

Agora verificando a qualidade desse modelo linear LASSO original.

```{r}

plot(lm_lasso_1)

df1 <- lm_lasso_1$residuals |> 
  as.data.frame() |> 
  rename(
    "residuos" = "lm_lasso_1$residuals"
  )

df1 |> 
  ggplot(aes(x = residuos)) +
  geom_histogram(aes(y =..density..),
                 breaks = seq(-3, 3, by = 0.25), 
                 colour = "black", 
                 fill = "white") +
  stat_function(fun = dnorm,
                args = list(mean = mean(df1$residuos),
                            sd   = sd  (df1$residuos)),
                color = "blue", size = 0.1) +
  geom_density(aes(x = df1$residuos)) +
  scale_x_continuous(breaks = seq(-3, 3, by = 1.0))

olsrr::ols_plot_cooksd_chart(lm_lasso_1)

olsrr::ols_plot_resid_lev(lm_lasso_1)
```

Agora um teste de normalidade dos resíduos desse modelo: `lm_lasso_1`.

```{r}

ad.test(df1$residuos)

shapiro.test(df1$residuos)

ks.test(df1$residuos, rnorm(n = 298,
                            mean = 0.0,
                            sd = sd(df1$residuos)) )
```

Eis agora o ***modelo linerar de regressão múltipla com essas 12 variáveis*** selecionadas pelo método LASSO.

```{r}

lm_lasso_2 <- lm(obitj[, colunas] |> 
                filter(idadeobta >  2) |> 
                filter(temposesc >= 0) |>
                na.omit(),
                formula = idadeobta ~ cor + corag + esc1 + esc2 + esc3 + esc4 + compfam + relpai + usudrog + subst + npassag + temposesc,
                na.action = na.omit)

summary(lm_lasso_2)

AIC(lm_lasso_2)

plot(lm_lasso_2)

df2 <- lm_lasso_2$residuals |> 
  as.data.frame() |> 
  rename(
    "residuos" = "lm_lasso_2$residuals"
  )

df2 |> 
  ggplot(aes(x = residuos)) +
  geom_histogram(aes(y =..density..),
                 breaks = seq(-3, 3, by = 0.25), 
                 colour = "black", 
                 fill = "white") +
  stat_function(fun = dnorm,
                args = list(mean = mean(df2$residuos),
                            sd   = sd  (df2$residuos)),
                color = "blue", size = 0.1) +
  geom_density(aes(x = df2$residuos)) +
  scale_x_continuous(breaks = seq(-3, 3, by = 1.0))

olsrr::ols_plot_cooksd_chart(lm_lasso_2)

olsrr::ols_plot_resid_lev(lm_lasso_2)
```

Agora um teste de normalidade dos resíduos desse modelo: `lm_lasso_2`.

```{r}

ad.test(df2$residuos)

shapiro.test(df2$residuos)

ks.test(df2$residuos, rnorm(n = 298,
                            mean = 0.0,
                            sd = sd(df2$residuos)) )
```

Todavia esse teste, com valor-p = 0.04158 = 4.2% \< 5% (Erro Tipo I), indica que a estatística de teste caiu na Região de Rejeição, o que leva à ***rejeição*** da hipótese nula: Ho - 𝑂𝑠 𝑑𝑎𝑑𝑜𝑠 𝑠𝑒𝑔𝑢𝑒𝑚 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑖çã𝑜 𝑁𝑜𝑟𝑚𝑎𝑙*.* Sendo que a estatpistica de teste caiu no início da Região de Rejeição.

Contudo, O teste de Shapiro-Wilk não leva à rejeição dessa hipótese nula (NC = 95%), com valor-p = 0.1391 = 13.9% \> 5% (Erro Tipo I).

O mesmo ocorrendo com o teste de Kolmogorov-Smirnov, que considera as duas distribuições testadas contínuas, exibiu um valor-p = 0.9393 = 93.9% \> 5%, caindo fora da região de rejeição; por esse teste *não* se deveria rejeitar a hipótese nula de nenhuma diferença entre as duas distribuições testadas: os resíduos do modelo e uma distribuição normal com média zero e desvio padrão igual ao desvio padrão observado nos dados: `sd = 0.8448851 anos`.

Ou seja, há fraca evidência a favor da hipótese alternativa, de que, com 95% de confiança, os resíduos do último modelo escolhido `lm_lasso_2` ***não*** seguem uma distribuição normal, que é um pressuposto de um modelo de regressão linear.

Para comparar esses 3 últimos modelos, recorrer a um teste de ANOVA.

```{r}

# anova(lasso.mod) # equivale ao lm_lasso_1
cat("Anova lm_lasso_1\n")
anova(lm_lasso_1)

cat("\n")
cat("\n")
cat("Anova lm_lasso_2\n")
cat("não é rigorosamente uma modelagem lasso.\n")
anova(lm_lasso_2) # não é rigorosamente uma modelagem lasso

# Combinar LASSO com regressão ridge: Elastic Net
cat("\n")
cat("\n")
cat("Combinar LASSO com regressão ridge: Elastic Net\n")
cat("\n")
set.seed(42)
cv_5 <- caret::trainControl(method = "cv", number = 5)

lm_lasso_enet <- caret::train(
  idadeobta ~ ., data = obitj_red,
  method = "glmnet",
  trControl = cv_5,
  tuneLength = 10
)

lm_lasso_enet

get_best_result <- function(caret_fit) {
  best <- which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result <- caret_fit$results[best, ]
  rownames(best_result) <- NULL
  best_result
}

get_best_result(lm_lasso_enet)

fit_lasso_cv <- cv.glmnet(x, y, alpha = 1)
cat("\n")
cat("\n")
cat("CV-RMSE minimum\n")
sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min]) # CV-RMSE minimum

cat("\n")
cat("\n")
cat("Modelo escolhido lm_lasso_enet:\n")
cat("Combina LASSO com regressão ridge: Elastic Net\n")
fit_lasso_cv

cat("\n")
cat("\n")
cat("Coeficientes de lm_lasso_enet:\n")
coef(fit_lasso_cv)

coef_fit_lasso_cv <- coef(fit_lasso_cv)[coef(fit_lasso_cv) != 0]

names(coef_fit_lasso_cv) <- c("Intercept", "esc12serieEM", "escn4n", "usudrogn", "napassag", "temposesc")

cat("\n")
cat("\n")
cat("Coeficientes não nulos de lm_lasso_enet:\n")
coef_fit_lasso_cv

cat("\n")
cat("\n")
cat("Número de coeficientes não nulos de lm_lasso_enet:\n")
sum(coef(fit_lasso_cv) != 0)
```

Com isso podemos considerar alcançado o ousado objetivo da 1ª turma desta disciplina de Núcleo Livre **EBRADPP**.

### Até breve

Dúvidas serão debeladas a cada aula!

![Até nosso pRRRóximo RRRencontro!](fig/ValeuGalera.png){fig-align="center"}
